{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mojtabaSefidi/Fall-Detection-System/blob/master/Fall_Detection_System_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4Hyqy-KZ3Eu",
        "outputId": "f446e156-6a60-473c-b1ce-530dcde0542e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIWFK9t8bVk1"
      },
      "source": [
        "### Imports and installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MwqF72o5A4-2"
      },
      "outputs": [],
      "source": [
        "!pip install -q tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "qhvZxm9n-jmF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import compute_class_weight\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from itertools import groupby, chain\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from math import sqrt\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import glob\n",
        "import os\n",
        "sn.set()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 200"
      ],
      "metadata": {
        "id": "qQ54WqPC91ZF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPXpiZUY6HBc"
      },
      "source": [
        "### Additional Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mw2f3Oyj7z4b"
      },
      "outputs": [],
      "source": [
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "\n",
        "def plot_confusion_matrix(confusion_matrix, title='', cmap ='Purples'):\n",
        "    df = pd.DataFrame(confusion_matrix, range(len(confusion_matrix)), range(len(confusion_matrix)))\n",
        "    plt.figure(figsize=(6,3))\n",
        "    if title == '' :\n",
        "        plt.title('Confusion Matrix')\n",
        "    else:\n",
        "        plt.title('Confusion Matrix for' + ' ' + title)\n",
        "    sn.set(font_scale=1) # for label size\n",
        "    sn.heatmap(df, annot=True, annot_kws={\"size\": 12},fmt='.0f',cmap=cmap) # font size\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_precision_recall_curve(actual_labels, prediction, title='', file_name=None):\n",
        "    precision, recall, thresholds=precision_recall_curve(actual_labels, prediction)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(recall, precision, color='purple')\n",
        "\n",
        "    # add axis labels to plot\n",
        "    if title == '':\n",
        "        plt.title(title)\n",
        "    else:\n",
        "        plt.title('Precision-Recall Curve')\n",
        "    ax.set_ylabel('Precision')\n",
        "    ax.set_xlabel('Recall')\n",
        "\n",
        "    # display plot\n",
        "    plt.show()\n",
        "    if file_name is not None:\n",
        "        plt.savefig(file_name)\n",
        "\n",
        "def plot_roc_curve(actual_labels, prediction, title='', file_name=None):\n",
        "    fpr, tpr, _ = roc_curve(actual_labels, prediction)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot(fpr, tpr)\n",
        "    if title == '':\n",
        "        plt.title(title)\n",
        "    else:\n",
        "        plt.title('ROC Learning Curves')\n",
        "    plt.xlabel('false positive rate')\n",
        "    plt.ylabel('true positive rate')\n",
        "    plt.show()\n",
        "\n",
        "    if file_name is not None:\n",
        "        plt.savefig(file_name)\n",
        "\n",
        "\n",
        "def plot_metrics(history):\n",
        "    metrics = ['loss', 'PRC', 'Precision', 'Recall']\n",
        "    plt.figure(figsize=(10,10),linewidth = 7, edgecolor=\"whitesmoke\")\n",
        "\n",
        "    for n, metric in enumerate(metrics):\n",
        "        name = metric.replace(\"_\",\" \").capitalize()\n",
        "        plt.subplot(2,2,n+1)\n",
        "        plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
        "        plt.plot(history.epoch, history.history['val_'+metric],\n",
        "             color=colors[0], linestyle=\"--\", label='Val')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel(name)\n",
        "    if metric == 'loss':\n",
        "        plt.ylim([0, plt.ylim()[1]])\n",
        "    elif metric == 'auc':\n",
        "        plt.ylim([0.8,1])\n",
        "    else:\n",
        "        plt.ylim([0,1])\n",
        "\n",
        "    plt.legend()\n",
        "\n",
        "def plot_auc_curve(actual_labels, prediction, model_name, title='', file_name = None):\n",
        "    fpr, tpr, _ = roc_curve(actual_labels,  prediction)\n",
        "    auc = roc_auc_score(actual_labels, prediction).round(4)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    if title == '':\n",
        "        plt.title(title)\n",
        "    else:\n",
        "        plt.title('AUC Learning Curves')\n",
        "    plt.plot(fpr,tpr, label='Model: '+ model_name + \", AUC=\" + str(auc), color='red')\n",
        "    plt.legend(loc=4)\n",
        "    plt.show()\n",
        "    if file_name is not None:\n",
        "        plt.savefig(file_name)\n",
        "\n",
        "def plot_history(history):\n",
        "\n",
        "    plt.figure(figsize=(10,5),linewidth = 7, edgecolor=\"whitesmoke\")\n",
        "    n = len(history.history['Accuracy'])\n",
        "\n",
        "    plt.plot(np.arange(0,n)+1,history.history['Accuracy'], color='orange',marker=\".\")\n",
        "    plt.plot(np.arange(0,n)+1,history.history['loss'],'b',marker=\".\")\n",
        "\n",
        "    # offset both validation curves\n",
        "    plt.plot(np.arange(0,n)+ 1,history.history['val_Accuracy'],'r')\n",
        "    plt.plot(np.arange(0,n)+ 1,history.history['val_loss'],'g')\n",
        "\n",
        "    plt.legend(['Train Acc','Train Loss','Val Acc','Val Loss'])\n",
        "    plt.grid(True)\n",
        "\n",
        "    # set vertical limit to 1\n",
        "    plt.gca().set_ylim(0,1)\n",
        "\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.suptitle(\"Learning Curve\", size=16, y=0.927)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCOqfWlyqoI3"
      },
      "source": [
        "## Pre-Processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Rh4rp7_Z_i0q"
      },
      "outputs": [],
      "source": [
        "class DatasetProcessor():\n",
        "\n",
        "  def get_file_name(self, path, ratio=0.8):\n",
        "    allfiles = []\n",
        "    allFolders = sorted(glob.glob(path + \"*\"))\n",
        "    for files in allFolders:\n",
        "      allfiles.append(sorted(glob.glob(files+\"/*.txt\")))\n",
        "    if 'desktop.ini' in allfiles:\n",
        "          allfiles.remove('desktop.ini')\n",
        "\n",
        "    dataset = np.hstack(allfiles)\n",
        "    start = dataset[0].rfind('/') + 1\n",
        "    end = dataset[0][start:].find('_') + start\n",
        "    dataset = [list(g) for k, g in groupby(dataset, key=lambda x: x[start:end])]\n",
        "    train = []\n",
        "    test = []\n",
        "    for data in dataset:\n",
        "      if len(data) == 1:\n",
        "        if random.randint(1,100)>=81:\n",
        "          test.extend(data)\n",
        "        else:\n",
        "          train.extend(data)\n",
        "\n",
        "      else:\n",
        "        random.shuffle(data)\n",
        "        train.extend(data[:int(len(data)*ratio)])\n",
        "        test.extend(data[int(len(data)*ratio):])\n",
        "\n",
        "    return train, test\n",
        "\n",
        "  def __read_data(self, data_path):\n",
        "    data = pd.read_csv(data_path, header=None)\n",
        "    data.columns = ['ADXL345_x', 'ADXL345_y', 'ADXL345_z', 'ITG3200_x', 'ITG3200_y', 'ITG3200_z', 'MMA8451Q_x',\n",
        "                    'MMA8451Q_y', 'MMA8451Q_z']\n",
        "    data['MMA8451Q_z'] = data['MMA8451Q_z'].map(lambda x: str(x)[:-1])\n",
        "    for name in data.columns :\n",
        "      data[name] = data[name].astype(float)\n",
        "    return data\n",
        "\n",
        "  def __add_label(self, data_path, merge_feature=False):\n",
        "\n",
        "    dataset = self.__read_data(data_path)\n",
        "\n",
        "    if not merge_feature:\n",
        "      dataset['label'] = self.__get_label(data_path)\n",
        "      return dataset.to_numpy()\n",
        "\n",
        "    else:\n",
        "      new_dataset = pd.DataFrame()\n",
        "      new_dataset['acc_1'] = dataset.apply(\n",
        "          lambda row: sqrt((row.ADXL345_x ** 2 + row.ADXL345_y ** 2 + row.ADXL345_z ** 2)), axis=1)\n",
        "      new_dataset['acc_2'] = dataset.apply(\n",
        "          lambda row: sqrt((row.MMA8451Q_x ** 2 + row.MMA8451Q_y ** 2 + row.MMA8451Q_z ** 2)), axis=1)\n",
        "      new_dataset['geo'] = dataset.apply(\n",
        "          lambda row: sqrt((row.ITG3200_x ** 2 + row.ITG3200_y ** 2 + row.ITG3200_z ** 2)), axis=1)\n",
        "      new_dataset['label'] = self.__get_label(data_path)\n",
        "\n",
        "      return np.round(new_dataset.to_numpy(), 2)\n",
        "\n",
        "  def __get_label(self, data_path):\n",
        "    label = data_path[54]\n",
        "    if label =='D':\n",
        "      return int(0)\n",
        "    elif label =='F':\n",
        "      label_path = data_path.replace('dataset', 'enhanced')\n",
        "      labels = pd.read_csv(label_path, header=None)\n",
        "      return labels\n",
        "\n",
        "  def datasets_to_nparray(self, datasets_address_array, outputsize=20000000, column_dimension=10):\n",
        "    result = np.zeros((outputsize, column_dimension), 'int16')\n",
        "    first_index = 0\n",
        "    for address in tqdm(datasets_address_array, ncols=50):\n",
        "      feature = self.__add_label(address)\n",
        "      result[first_index : (first_index+len(feature))] = feature\n",
        "      first_index += len(feature)\n",
        "\n",
        "    return result[result.sum(axis=1) != 0]\n",
        "\n",
        "  def windowing2d(self, dataset, window_size=200):\n",
        "    window = window_size * (dataset.shape[1]-1)\n",
        "    cut = dataset.shape[0] % window_size\n",
        "    feature = dataset[:-cut,0:-1]\n",
        "    label = dataset[:-cut,-1]\n",
        "    feature = feature.ravel().reshape(feature.size//window,window)\n",
        "    label = label.reshape(label.size// window_size, window_size)\n",
        "    label = label.sum(axis=1)\n",
        "    label[label > 0] = 1\n",
        "    feature = np.roll((np.roll(feature, -1, axis=0) - feature), 1, axis=0)\n",
        "    feature[0] = 0\n",
        "    return feature, label.ravel()\n",
        "\n",
        "  def windowing3d(self, dataset, window_size=200):\n",
        "    n_windows = len(dataset) // window_size\n",
        "    cut = dataset.shape[0] % window_size\n",
        "    feature = dataset[:-cut,0:-1]\n",
        "    label = dataset[:-cut,-1]\n",
        "    feature = feature.reshape(n_windows, window_size, dataset.shape[1]-1)\n",
        "    label = label.reshape(n_windows, window_size, 1)\n",
        "    label = label.sum(axis=1)\n",
        "    label[label > 0] = 1\n",
        "    feature = np.roll((np.roll(feature, -1, axis=0) - feature), 1, axis=0)\n",
        "    feature[0] = 0\n",
        "    return feature, label.ravel()\n",
        "\n",
        "  def normalizer(self, scaler, X_train, X_test):\n",
        "    X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
        "    X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
        "    return X_train, X_test\n",
        "\n",
        "  def dataset_to_tensor(self, window_size, dataset, saving_path):\n",
        "    features, labels = self.windowing(self.__datasets_to_nparray(dataset), window_size)\n",
        "    return features, labels\n",
        "\n",
        "  def downsampling(self, dataset, down_sampleing_factor):\n",
        "      positive = dataset[dataset['targets']==1]\n",
        "      negative = dataset[dataset['targets']==0].sample(n=int(len(positive)* down_sampleing_factor))\n",
        "      return pd.concat([positive, negative], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  def generate_class_weight(self, label):\n",
        "    class_weights = compute_class_weight(class_weight = \"balanced\",\n",
        "                                         classes = np.unique(label),\n",
        "                                         y = label)\n",
        "    return dict(zip(np.unique(label), class_weights))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "_1pcZKGXL-OM",
        "outputId": "66391140-a941-4315-f7ac-c4ad0d6e8a64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.Split Adresses...\n",
            "2.Extract Features and Labels...\n",
            "------------------------Train Dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|â–‰         | 327/3508 [00:39<24:38,  2.15it/s]"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "dp = DatasetProcessor()\n",
        "print('1.Split Adresses...')\n",
        "train, test = dp.get_file_name('/content/gdrive/MyDrive/Datasets/SisFall_dataset/')\n",
        "\n",
        "print('2.Extract Features and Labels...')\n",
        "print('------------------------Train Dataset')\n",
        "train_dataset = dp.datasets_to_nparray(train)\n",
        "print('------------------------Test Dataset')\n",
        "test_dataset = dp.datasets_to_nparray(test)\n",
        "\n",
        "print('3.Windowing...')\n",
        "print('------------------------Train Dataset')\n",
        "X_train, y_train = dp.windowing3d(train_dataset)\n",
        "print('------------------------Test Dataset')\n",
        "X_test, y_test = dp.windowing3d(test_dataset)\n",
        "\n",
        "print('4.Normalizing...')\n",
        "scaler = StandardScaler()\n",
        "X_train, X_test = dp.normalizer(scaler, X_train, X_test)\n",
        "\n",
        "print('5.Calculate Class Weight...')\n",
        "class_weight = dp.generate_class_weight(y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FcPKDH_MSoi"
      },
      "source": [
        "### Save/Load train & test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5X6ABVhhLfNk"
      },
      "outputs": [],
      "source": [
        "save = False\n",
        "if save:\n",
        "  with open('/content/gdrive/MyDrive/Fall Detection System/Dataset/X_train','wb') as f:\n",
        "    pickle.dump(X_train, f)\n",
        "\n",
        "  with open('/content/gdrive/MyDrive/Fall Detection System/Dataset/y_train.pkl','wb') as f:\n",
        "    pickle.dump(y_train, f)\n",
        "\n",
        "  with open('/content/gdrive/MyDrive/Fall Detection System/Dataset/X_test.pkl','wb') as f:\n",
        "    pickle.dump(X_test, f)\n",
        "\n",
        "  with open('/content/gdrive/MyDrive/Fall Detection System/Dataset/y_test.pkl','wb') as f:\n",
        "    pickle.dump(y_test, f)\n",
        "\n",
        "else:\n",
        "  with open('/content/gdrive/MyDrive/Fall Detection System/Dataset/X_train.pkl','rb') as f:\n",
        "    X_train = pickle.load(f)\n",
        "\n",
        "  with open('/content/gdrive/MyDrive/Fall Detection System/Dataset/y_train.pkl','rb') as f:\n",
        "    y_train = pickle.load(f)\n",
        "\n",
        "  with open('/content/gdrive/MyDrive/Fall Detection System/Dataset/X_test.pkl','rb') as f:\n",
        "    X_test = pickle.load(f)\n",
        "\n",
        "  with open('/content/gdrive/MyDrive/Fall Detection System/Dataset/y_test.pkl','rb') as f:\n",
        "    y_test = pickle.load(f)\n",
        "    dp = DatasetProcessor()\n",
        "    class_weight = dp.generate_class_weight(y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qp2AhaTKkGxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "475c0d9f-3d8a-4988-f725-916ec1c18d4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (61036, 200, 9)\n",
            "y_train shape: (61036,)\n",
            "X_test shape: (16375, 200, 9)\n",
            "y_test shape: (16375,)\n"
          ]
        }
      ],
      "source": [
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'y_train shape: {y_train.shape}')\n",
        "print(f'X_test shape: {X_test.shape}')\n",
        "print(f'y_test shape: {y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXsG6DAmfm7m"
      },
      "source": [
        "## Train & Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_g6IkRVaDk26"
      },
      "outputs": [],
      "source": [
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='TP'),\n",
        "      keras.metrics.FalsePositives(name='FP'),\n",
        "      keras.metrics.TrueNegatives(name='TN'),\n",
        "      keras.metrics.FalseNegatives(name='FN'),\n",
        "      keras.metrics.BinaryAccuracy(name='Accuracy'),\n",
        "      keras.metrics.Precision(name='Precision'),\n",
        "      keras.metrics.Recall(name='Recall'),\n",
        "      keras.metrics.AUC(name='AUC'),\n",
        "      keras.metrics.AUC(name='PRC', curve='PR')\n",
        "]\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='Precision',\n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    mode='max',\n",
        "    restore_best_weights=True)\n",
        "\n",
        "# checkpoint_path = \"/content/gdrive/MyDrive/Model Checkpoints/TrendDetection.ckpt\"\n",
        "# # save_freq = 'epoch'\n",
        "\n",
        "# cp_callback = tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "#    checkpoint_path, verbose=1, save_weights_only=True,\n",
        "#    # Save weights, every epoch.\n",
        "#    save_freq = 10*3030)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "lXXuUHUODbJL"
      },
      "outputs": [],
      "source": [
        "class Train_Evaluate():\n",
        "\n",
        "  def build_cnn(self, input_size, units=128, drop_rate=0.25, filter=32, kernel_size=(1*9), output_size=1):\n",
        "    input = keras.layers.Input((input_size))\n",
        "    x = keras.layers.Conv1D(filters=filter, kernel_size=kernel_size, padding='same', activation='relu', name=\"conv1\")(input)\n",
        "    # x = keras.layers.MaxPooling1D(padding=\"same\")(x)\n",
        "    x = keras.layers.Conv1D(filters=filter*2, kernel_size=kernel_size, padding='same', activation='relu', name=\"conv2\")(x)\n",
        "    # x = keras.layers.MaxPooling1D(padding=\"same\")(x)\n",
        "    x = keras.layers.Conv1D(filters=filter*4, kernel_size=kernel_size, padding='same', activation='relu', name=\"conv3\")(x)\n",
        "    # x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    classifier = keras.layers.Dense(units*4, activation='relu')(x)\n",
        "    classifier = keras.layers.Dropout(drop_rate)(classifier)\n",
        "    classifier = keras.layers.Dense(units, activation='relu')(classifier)\n",
        "    classifier = keras.layers.Dropout(drop_rate)(classifier)\n",
        "    classifier = keras.layers.Dense(units//2, activation='relu')(classifier)\n",
        "    classifier = keras.layers.Dropout(drop_rate)(classifier)\n",
        "    output = keras.layers.Dense(output_size, activation='sigmoid')(classifier)\n",
        "    model = keras.Model(inputs=input, outputs=output)\n",
        "    return model\n",
        "\n",
        "  def build_lstm(self, input_size, units=128, drop_rate=0.25, lstm_units=16, output_size=1):\n",
        "    input = keras.layers.Input((input_size))\n",
        "    x = keras.layers.LSTM(units=lstm_units, input_shape=input_size, return_sequences=True, name=\"lstm1\")(input)\n",
        "    x = keras.layers.LSTM(units=lstm_units*4, input_shape=input_size, return_sequences=True, name=\"lstm2\")(x)\n",
        "    # x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    classifier = keras.layers.Dense(units*4, activation='relu')(x)\n",
        "    classifier = keras.layers.Dropout(drop_rate)(classifier)\n",
        "    classifier = keras.layers.Dense(units, activation='relu')(classifier)\n",
        "    classifier = keras.layers.Dropout(drop_rate)(classifier)\n",
        "    classifier = keras.layers.Dense(units//2, activation='relu')(classifier)\n",
        "    classifier = keras.layers.Dropout(drop_rate)(classifier)\n",
        "    output = keras.layers.Dense(output_size, activation='sigmoid')(classifier)\n",
        "    model = keras.Model(inputs=input, outputs=output)\n",
        "    return model\n",
        "\n",
        "  def build_mlp(self, input_size, hidden_layer_size=128, output_size=1, drop_rate=0.25):\n",
        "    input = keras.layers.Input((input_size))\n",
        "    x = keras.layers.Dense(hidden_layer_size//2, activation='relu')(input)\n",
        "    x = keras.layers.Dense(hidden_layer_size, activation='relu')(input)\n",
        "    x = keras.layers.Dense(hidden_layer_size//2, activation='relu')(x)\n",
        "    x = keras.layers.Dense(hidden_layer_size//4, activation='relu')(x)\n",
        "    x = keras.layers.Dense(hidden_layer_size//16, activation='relu')(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    classifier = keras.layers.Dense(hidden_layer_size*4, activation='relu')(x)\n",
        "    classifier = keras.layers.Dropout(drop_rate)(classifier)\n",
        "    classifier = keras.layers.Dense(hidden_layer_size, activation='relu')(classifier)\n",
        "    classifier = keras.layers.Dropout(drop_rate)(classifier)\n",
        "    classifier = keras.layers.Dense(hidden_layer_size//2, activation='relu')(classifier)\n",
        "    classifier = keras.layers.Dropout(drop_rate)(classifier)\n",
        "    output = keras.layers.Dense(output_size, activation='sigmoid')(classifier)\n",
        "    model = keras.Model(inputs=input, outputs=output)\n",
        "    output = keras.layers.Dense(output_size, activation='sigmoid')(classifier)\n",
        "    model = keras.Model(inputs=input, outputs=output)\n",
        "    return model\n",
        "\n",
        "\n",
        "  def train_deep_model(self, model, X_train, y_train, class_weight, epochs=100, batch_size=256, learning_rate=0.001, validation_split=0.2):\n",
        "\n",
        "    print('----------------------------------')\n",
        "    print(model.summary())\n",
        "    print('----------------------------------')\n",
        "\n",
        "    model.compile(optimizer = tf.optimizers.SGD(learning_rate=learning_rate),\n",
        "                  loss = keras.losses.BinaryCrossentropy(),\n",
        "                  metrics=METRICS)\n",
        "\n",
        "    history = model.fit(X_train,\n",
        "                        y_train,\n",
        "                        batch_size = batch_size,\n",
        "                        epochs = epochs,\n",
        "                        shuffle = True,\n",
        "                        class_weight = class_weight,\n",
        "                        validation_split=validation_split,\n",
        "                        callbacks = [early_stopping],\n",
        "                        verbose = 1)\n",
        "    return history\n",
        "\n",
        "  def evaluate(self, model, X_test, Y_train, batch_size=256, threshold=0.5, title='', model_name='', plot=True):\n",
        "    prediction = model.predict(X_test, batch_size=batch_size)\n",
        "    prediction = np.where(prediction >= threshold, 1, 0)\n",
        "    print(classification_report(y_test, prediction))\n",
        "    plot_confusion_matrix(confusion_matrix(y_test, prediction), title=title)\n",
        "    if plot:\n",
        "      print('1) Plot ROC Curve...')\n",
        "      plot_roc_curve(y_test, prediction, title='ROC Curve Of Model'+title, file_name=None)\n",
        "      print('2) Plot AUC Curve...')\n",
        "      plot_auc_curve(y_test, prediction, model_name=model_name, title='AUC Curve Of Model'+title, file_name = None)\n",
        "      print('3) Plot Percision_Recall Curve......')\n",
        "      plot_precision_recall_curve(y_test, prediction, title='Percision_Recall Curve Of Model'+title, file_name=None)\n",
        "    return prediction\n",
        "\n",
        "  def plot_learning_curves(self, history):\n",
        "    print('1) Plot learning process based on different metrics...')\n",
        "    plot_metrics(history)\n",
        "    print('2) Plot learning curve...')\n",
        "    return plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_evaluation = Train_Evaluate()\n",
        "\n",
        "cnn_model = train_evaluation.build_cnn(input_size=X_train.shape[1:])\n",
        "history = train_evaluation.train_deep_model(cnn_model, X_train, y_train, class_weight, epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "id": "rgyXc1aC-tRI",
        "outputId": "a976da48-bb5b-4547-9790-cb0811efbe6b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------\n",
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_9 (InputLayer)        [(None, 200, 9)]          0         \n",
            "                                                                 \n",
            " conv1 (Conv1D)              (None, 200, 32)           2624      \n",
            "                                                                 \n",
            " conv2 (Conv1D)              (None, 200, 128)          36992     \n",
            "                                                                 \n",
            " conv3 (Conv1D)              (None, 200, 512)          590336    \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 102400)            0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 128)               13107328  \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13745601 (52.44 MB)\n",
            "Trainable params: 13745601 (52.44 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "----------------------------------\n",
            "Epoch 1/2\n",
            " 10/191 [>.............................] - ETA: 20:48 - loss: 0.7145 - TP: 1046.0000 - FP: 27821.0000 - TN: 2385.0000 - FN: 156.0000 - Accuracy: 0.1092 - Precision: 0.0362 - Recall: 0.8702 - AUC: 0.7630 - PRC: 0.2295"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-dc44951f3696>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcnn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_evaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_evaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_deep_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-52-df48c46a3f1c>\u001b[0m in \u001b[0;36mtrain_deep_model\u001b[0;34m(self, model, X_train, y_train, class_weight, epochs, batch_size, learning_rate, validation_split)\u001b[0m\n\u001b[1;32m     63\u001b[0m                   metrics=METRICS)\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     history = model.fit(X_train,\n\u001b[0m\u001b[1;32m     66\u001b[0m                         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1781\u001b[0m                         ):\n\u001b[1;32m   1782\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    868\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1263\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1480\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_evaluation = Train_Evaluate()\n",
        "LSTM_model = train_evaluation.build_lstm(input_size=X_train.shape[1:])\n",
        "history = train_evaluation.train_deep_model(LSTM_model, X_train, y_train, class_weight, epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcdK39QnB8jd",
        "outputId": "e007a90d-866b-4146-f87d-3332a472886b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------\n",
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_10 (InputLayer)       [(None, 200, 9)]          0         \n",
            "                                                                 \n",
            " lstm1 (LSTM)                (None, 200, 16)           1664      \n",
            "                                                                 \n",
            " lstm2 (LSTM)                (None, 200, 64)           20736     \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 128)               1638528   \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1669249 (6.37 MB)\n",
            "Trainable params: 1669249 (6.37 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "----------------------------------\n",
            "Epoch 1/2\n",
            "191/191 [==============================] - 206s 811ms/step - loss: 0.7119 - TP: 3522.0000 - FP: 66704.0000 - TN: 9889.0000 - FN: 377.0000 - Accuracy: 0.1666 - Precision: 0.0502 - Recall: 0.9033 - AUC: 0.7817 - PRC: 0.2319 - val_loss: 0.7424 - val_TP: 125.0000 - val_FP: 9893.0000 - val_TN: 2189.0000 - val_FN: 1.0000 - val_Accuracy: 0.1895 - val_Precision: 0.0125 - val_Recall: 0.9921 - val_AUC: 0.8700 - val_PRC: 0.0732\n",
            "Epoch 2/2\n",
            "191/191 [==============================] - ETA: 0s - loss: 0.6540 - TP: 2633.0000 - FP: 28959.0000 - TN: 17189.0000 - FN: 47.0000 - Accuracy: 0.4060 - Precision: 0.0833 - Recall: 0.9825 - AUC: 0.8982 - PRC: 0.4276"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "jYkwD9PtaWWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e0801e4-b3aa-44a5-b43b-3ddcfc368742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------\n",
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_11 (InputLayer)       [(None, 200, 9)]          0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 200, 128)          1280      \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 200, 64)           8256      \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 200, 32)           2080      \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 200, 8)            264       \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 128)               204928    \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 225129 (879.41 KB)\n",
            "Trainable params: 225129 (879.41 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "----------------------------------\n",
            "Epoch 1/2\n",
            "191/191 [==============================] - 34s 163ms/step - loss: 0.6840 - TP: 2408.0000 - FP: 45095.0000 - TN: 13135.0000 - FN: 398.0000 - Accuracy: 0.2547 - Precision: 0.0507 - Recall: 0.8582 - AUC: 0.7548 - PRC: 0.2340 - val_loss: 0.8518 - val_TP: 126.0000 - val_FP: 9754.0000 - val_TN: 2328.0000 - val_FN: 0.0000e+00 - val_Accuracy: 0.2010 - val_Precision: 0.0128 - val_Recall: 1.0000 - val_AUC: 0.8558 - val_PRC: 0.0475\n",
            "Epoch 2/2\n",
            "191/191 [==============================] - 31s 162ms/step - loss: 0.5960 - TP: 2663.0000 - FP: 28696.0000 - TN: 17452.0000 - FN: 17.0000 - Accuracy: 0.4120 - Precision: 0.0849 - Recall: 0.9937 - AUC: 0.9036 - PRC: 0.3823 - val_loss: 0.9195 - val_TP: 126.0000 - val_FP: 8209.0000 - val_TN: 3873.0000 - val_FN: 0.0000e+00 - val_Accuracy: 0.3276 - val_Precision: 0.0151 - val_Recall: 1.0000 - val_AUC: 0.8708 - val_PRC: 0.0532\n"
          ]
        }
      ],
      "source": [
        "train_evaluation = Train_Evaluate()\n",
        "\n",
        "mlp = train_evaluation.build_mlp(input_size=X_train.shape[1:], hidden_layer_size=128, output_size=1)\n",
        "history = train_evaluation.train_deep_model(mlp, X_train, y_train, class_weight, epochs=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CzHQADbnzVt"
      },
      "source": [
        "Prediction based on Ensemble Concept"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT1qAAPVHRDD"
      },
      "source": [
        "Models' Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "E4-Ab6KrGkNv",
        "outputId": "37cb1cb3-3976-49d6-d813-eb690ea4ed86"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row0_col0,#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row6_col1,#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row7_col2{\n",
              "            background-color:  #800026;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row0_col1,#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row2_col0{\n",
              "            background-color:  #febb56;\n",
              "            color:  #000000;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row0_col2{\n",
              "            background-color:  #f03523;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row1_col0{\n",
              "            background-color:  #fd8a3b;\n",
              "            color:  #000000;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row1_col1{\n",
              "            background-color:  #ffe793;\n",
              "            color:  #000000;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row1_col2{\n",
              "            background-color:  #fec05b;\n",
              "            color:  #000000;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row2_col1,#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row2_col2,#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row6_col0{\n",
              "            background-color:  #ffffcc;\n",
              "            color:  #000000;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row3_col0{\n",
              "            background-color:  #990026;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row3_col1{\n",
              "            background-color:  #fc6832;\n",
              "            color:  #000000;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row3_col2,#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row4_col2,#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row5_col1{\n",
              "            background-color:  #930026;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row4_col0{\n",
              "            background-color:  #b90026;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row4_col1{\n",
              "            background-color:  #fc5d2e;\n",
              "            color:  #000000;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row5_col0{\n",
              "            background-color:  #fffcc5;\n",
              "            color:  #000000;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row5_col2{\n",
              "            background-color:  #ffe691;\n",
              "            color:  #000000;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row6_col2{\n",
              "            background-color:  #ffefa4;\n",
              "            color:  #000000;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row7_col0{\n",
              "            background-color:  #e9261f;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row7_col1{\n",
              "            background-color:  #e2191c;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row8_col0{\n",
              "            background-color:  #fa4a29;\n",
              "            color:  #000000;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row8_col1{\n",
              "            background-color:  #c90823;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_611e1a9c_5135_11ec_be3e_0242ac1c0002row8_col2{\n",
              "            background-color:  #860026;\n",
              "            color:  #f1f1f1;\n",
              "        }</style><table id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002\" class=\"dataframe\"><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Precision</th>        <th class=\"col_heading level0 col1\" >Recall</th>        <th class=\"col_heading level0 col2\" >F1score</th>    </tr>    <tr>        <th class=\"index_name level0\" >Algorithm</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >Neural Network</th>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row0_col0\" class=\"data row0 col0\" >0.990000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row0_col1\" class=\"data row0 col1\" >0.340000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row0_col2\" class=\"data row0 col2\" >0.510000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >Logistic Regression</th>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row1_col0\" class=\"data row1 col0\" >0.530000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row1_col1\" class=\"data row1 col1\" >0.170000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row1_col2\" class=\"data row1 col2\" >0.260000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >SVM</th>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row2_col0\" class=\"data row2 col0\" >0.380000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row2_col1\" class=\"data row2 col1\" >0.010000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row2_col2\" class=\"data row2 col2\" >0.030000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >KNN</th>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row3_col0\" class=\"data row3 col0\" >0.940000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row3_col1\" class=\"data row3 col1\" >0.560000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row3_col2\" class=\"data row3 col2\" >0.700000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >Neural Network after Balancing</th>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row4_col0\" class=\"data row4 col0\" >0.880000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row4_col1\" class=\"data row4 col1\" >0.580000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row4_col2\" class=\"data row4 col2\" >0.700000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >Logistic Regression after Balancing</th>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row5_col0\" class=\"data row5 col0\" >0.080000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row5_col1\" class=\"data row5 col1\" >0.930000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row5_col2\" class=\"data row5 col2\" >0.150000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >SVM after Balancing</th>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row6_col0\" class=\"data row6 col0\" >0.060000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row6_col1\" class=\"data row6 col1\" >0.970000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row6_col2\" class=\"data row6 col2\" >0.110000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002level0_row7\" class=\"row_heading level0 row7\" >KNN after Balancing</th>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row7_col0\" class=\"data row7 col0\" >0.730000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row7_col1\" class=\"data row7 col1\" >0.730000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row7_col2\" class=\"data row7 col2\" >0.730000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002level0_row8\" class=\"row_heading level0 row8\" >Ensemble concept after Balancing</th>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row8_col0\" class=\"data row8 col0\" >0.650000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row8_col1\" class=\"data row8 col1\" >0.810000</td>\n",
              "                        <td id=\"T_611e1a9c_5135_11ec_be3e_0242ac1c0002row8_col2\" class=\"data row8 col2\" >0.720000</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fa5792cc150>"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conclusion = pd.DataFrame([['Neural Network',precision_1,recall_1,f1Score_1],\n",
        "              ['Logistic Regression',0.53,0.17,0.26],\n",
        "              ['SVM',0.38,0.01,0.03],\n",
        "              ['KNN',0.94,0.56,0.70],\n",
        "              ['Neural Network after Balancing',precision_2,recall_2,f1Score_2],\n",
        "              ['Logistic Regression after Balancing',0.08,0.93,0.15],\n",
        "              ['SVM after Balancing',0.06,0.97,0.11],\n",
        "              ['KNN after Balancing',0.73,0.73,0.73],\n",
        "              ['Ensemble concept after Balancing',0.65,0.81,0.72]],\n",
        "              columns=[\"Model\",\"Precision\",\"Recall\",\"F1score\"])\n",
        "# conclusion = conclusion.set_index('Algorithm')\n",
        "conclusion.style.background_gradient(cmap=\"YlGn\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAV6ArSEQ0xd"
      },
      "source": [
        "As you see After Blancing the dataset Ensemble concept and KNN model do the best !"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcUIZ/JXf7Fhbt2oeGD/Z9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}